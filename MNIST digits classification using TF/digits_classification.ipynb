{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in c:\\users\\hasee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from keras) (1.14.0)\n",
      "Collecting scipy>=0.14 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/61/51/046cbc61c7607e5ecead6ff1a9453fba5e7e47a5ea8d608cc7036586a5ef/scipy-1.4.1-cp37-cp37m-win_amd64.whl\n",
      "Collecting pyyaml (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/aa/1a7ac452c52b93ab759d0b5b81c901ea122d95a5abf429decc660a44a2f1/PyYAML-5.3.1-cp37-cp37m-win_amd64.whl (216kB)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04a1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in c:\\users\\hasee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from keras) (1.18.0)\n",
      "Installing collected packages: scipy, pyyaml, h5py, keras-applications, keras-preprocessing, keras\n",
      "Successfully installed h5py-2.10.0 keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 pyyaml-5.3.1 scipy-1.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras-utils\n",
    "!pip3 install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"We're using TF\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import matplotlib_utils\n",
    "from importlib import reload\n",
    "reload(matplotlib_utils)\n",
    "\n",
    "\n",
    "import keras_utils\n",
    "# from keras_utils import reset_tf_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessed_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X contains rgb values divided by 255\n",
    "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
    "print(\"A closeup of a sample patch:\")\n",
    "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"And the whole sample:\")\n",
    "plt.imshow(X_train[1], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n",
      "(50000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
    "print(X_train_flat.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
    "print(X_val_flat.shape)\n",
    "\n",
    "\n",
    "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
    "\n",
    "print(y_train_oh.shape)\n",
    "print(y_train_oh[:3], y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this again if you remake your graph\n",
    "# s = reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model parameters: W and b\n",
    "W = tf.get_variable('w',shape=(784,10),dtype=tf.float32)\n",
    "b = tf.get_variable('b',shape=[10],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder(tf.float32,(None,784))\n",
    "input_y = tf.placeholder(tf.float32,(None,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "logits = input_X@W + b \n",
    "probas = tf.nn.softmax(logits)\n",
    "classes = tf.argmax(probas,axis=1)\n",
    "\n",
    "# Loss should be a scalar number: average loss over all the objects with tf.reduce_mean().\n",
    "# Use tf.nn.softmax_cross_entropy_with_logits on top of one-hot encoded input_y and logits.\n",
    "# It is identical to calculating cross-entropy on top of probas, but is more numerically friendly\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=logits))\n",
    "\n",
    "# Use a default tf.train.AdamOptimizer to get an SGD step\n",
    "step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40\n",
    "\n",
    "# for logging the progress right here in Jupyter (for those who don't have TensorBoard)\n",
    "simpleTrainingCurves = matplotlib_utils.SimpleTrainingCurves(\"cross-entropy\", \"accuracy\")\n",
    "\n",
    "for epoch in range(EPOCHS):  # we finish an epoch when we've looked at all training samples\n",
    "    \n",
    "    batch_losses = []\n",
    "    for batch_start in range(0, X_train_flat.shape[0], BATCH_SIZE):  # data is already shuffled\n",
    "        _, batch_loss = s.run([step, loss], {input_X: X_train_flat[batch_start:batch_start+BATCH_SIZE], \n",
    "                                             input_y: y_train_oh[batch_start:batch_start+BATCH_SIZE]})\n",
    "        # collect batch losses, this is almost free as we need a forward pass for backprop anyway\n",
    "        batch_losses.append(batch_loss)\n",
    "\n",
    "    train_loss = np.mean(batch_losses)\n",
    "    val_loss = s.run(loss, {input_X: X_val_flat, input_y: y_val_oh})  # this part is usually small\n",
    "    train_accuracy = accuracy_score(y_train, s.run(classes, {input_X: X_train_flat}))  # this is slow and usually skipped\n",
    "    valid_accuracy = accuracy_score(y_val, s.run(classes, {input_X: X_val_flat}))  \n",
    "    simpleTrainingCurves.add(train_loss, val_loss, train_accuracy, valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usinge hidden layers (3-layer NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "hidden1 = tf.layers.dense(input_X, 256, activation=tf.nn.sigmoid)\n",
    "hidden2 = tf.layers.dense(hidden1, 256, activation=tf.nn.sigmoid)\n",
    "logits = tf.layers.dense(hidden2, 10)\n",
    "\n",
    "probas = tf.nn.softmax(logits)\n",
    "classes = tf.argmax(probas,axis=1)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=logits))\n",
    "\n",
    "# Use a default tf.train.AdamOptimizer to get an SGD step\n",
    "step = tf.train.AdamOptimizer().minimize(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40\n",
    "\n",
    "# for logging the progress right here in Jupyter (for those who don't have TensorBoard)\n",
    "simpleTrainingCurves = matplotlib_utils.SimpleTrainingCurves(\"cross-entropy\", \"accuracy\")\n",
    "\n",
    "for epoch in range(EPOCHS):  # we finish an epoch when we've looked at all training samples\n",
    "    \n",
    "    batch_losses = []\n",
    "    for batch_start in range(0, X_train_flat.shape[0], BATCH_SIZE):  # data is already shuffled\n",
    "        _, batch_loss = s.run([step, loss], {input_X: X_train_flat[batch_start:batch_start+BATCH_SIZE], \n",
    "                                             input_y: y_train_oh[batch_start:batch_start+BATCH_SIZE]})\n",
    "        # collect batch losses, this is almost free as we need a forward pass for backprop anyway\n",
    "        batch_losses.append(batch_loss)\n",
    "\n",
    "    train_loss = np.mean(batch_losses)\n",
    "    val_loss = s.run(loss, {input_X: X_val_flat, input_y: y_val_oh})  # this part is usually small\n",
    "    train_accuracy = accuracy_score(y_train, s.run(classes, {input_X: X_train_flat}))  # this is slow and usually skipped\n",
    "    valid_accuracy = accuracy_score(y_val, s.run(classes, {input_X: X_val_flat}))  \n",
    "    simpleTrainingCurves.add(train_loss, val_loss, train_accuracy, valid_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
